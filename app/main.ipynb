{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Match Project\n",
    "\n",
    "First of all we import all we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import (FastAPI, \n",
    "                     File, \n",
    "                     UploadFile, \n",
    "                     HTTPException, \n",
    "                     Request)\n",
    "from fastapi.responses import JSONResponse\n",
    "import cv2\n",
    "import numpy as np\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from PIL import Image, ImageOps, ImageEnhance, ImageFilter\n",
    "from torchvision import transforms\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the app object and then we write all the allowed image file types. \n",
    "Then we define our models. resnet is face recognition model and the other ones are for face detection. \n",
    "\n",
    "> **Very Important Note**: Wherever I use the word \"__anchor__\" in my code, I point out the Licence image. On the other hand, wherever i use the word \"__data__\" I point out to the pictures which have taken in the bus and we are eager to recognize them by compare them with the anchor images(which are license images as I mentioned).The reason for this distinguish is that we don't want to preprocess the license images. However, the pictures which are taken inb the bus (dara images) need a lot of preprocessing!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "allowed_image_types = [\"image/jpeg\", \"image/png\", \"image/gif\"]\n",
    "resnet = InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
    "face_detector_data = cv2.FaceDetectorYN.create(\n",
    "                    model=r\"yunet.onnx\",\n",
    "                    config=\"\",\n",
    "                    input_size=(320, 240),\n",
    "                    score_threshold=0.9, \n",
    "                    nms_threshold=0.3,\n",
    "                    top_k=5000\n",
    "                )\n",
    "face_detector_anchor = cv2.FaceDetectorYN.create(\n",
    "                    model=r\"yunet.onnx\",\n",
    "                    config=\"\",\n",
    "                    input_size=(240, 320),\n",
    "                    score_threshold=0.9, \n",
    "                    nms_threshold=0.3,\n",
    "                    top_k=5000\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptions\n",
    "we will use these exceptions to return a meanongful error to the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class notReceivedException(Exception):\n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "@app.exception_handler(notReceivedException)\n",
    "async def notReceived_exception_handler(request: Request, exc:notReceivedException):\n",
    "    return JSONResponse(\n",
    "        status_code=404,\n",
    "        content={\"message\": f\"Oops! {exc.name} didn't receive!\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "We are defining some functions. We use coroutines to make our code more efficient and run effectively!\n",
    "1. first function is **grayscale_to_3channels(image)** which converts single channeled image to 3 channel and it's very simple as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def grayscale_to_3channels(image):\n",
    "    \"\"\"\n",
    "    Converts a grayscale PIL Image \n",
    "    to a 3-channel image. \n",
    "    \"\"\"\n",
    "    \n",
    "    if image.mode != 'L':\n",
    "        raise ValueError(\"Image must be in grayscale mode (L)\")\n",
    "\n",
    "    return Image.merge(\"RGB\", (image, image, image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. These two functions are for converting cv2 images to PIL images and vice versa! This is because we want to use ready codes used in the libraries and avoid writing them ourselves in sake of simplicity and efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def convert2cv2(pil_image):\n",
    "    \"\"\"\n",
    "    converts PIL images\n",
    "    to opencv images\n",
    "    \"\"\"\n",
    "    image = pil_image.convert('RGB')\n",
    "    opencvImage=np.array(image)\n",
    "    if opencvImage is None or np.all(opencvImage==0):\n",
    "        return np.zeros(shape=(150, 150, 3))\n",
    "    opencvImage=cv2.cvtColor(opencvImage, cv2.COLOR_BGR2RGB)\n",
    "    return opencvImage\n",
    "\n",
    "async def convert2PIL(cv2_image):\n",
    "    \"\"\"\n",
    "    converts opencv image \n",
    "    to PIL image.\n",
    "    \"\"\"\n",
    "    if cv2_image is None:\n",
    "        raise ValueError(\"File is empty.\")\n",
    "    \n",
    "    img = cv2.cvtColor(cv2_image, cv2.COLOR_BGR2RGB)\n",
    "    # opencvImage = cv2_image[:, :, ::-1].copy()\n",
    "    pil_img = Image.fromarray(img)\n",
    "\n",
    "    # For reversing the operation:\n",
    "    # pil_img = np.asarray(pil_img)\n",
    "    return pil_img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. These are some preprocesses that we exert on images. Function's names can say all about the preprocesses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rotate_180(image_array):\n",
    "    \"\"\"\n",
    "    rotate the image\n",
    "    180 degrees.\n",
    "    \"\"\"\n",
    "    return np.rot90(np.rot90(image_array))\n",
    "\n",
    "async def normalize_illumination(image):\n",
    "    \"\"\"\n",
    "    Normalizes the illumination \n",
    "    of an image using PIL.\n",
    "    \"\"\"\n",
    "\n",
    "    img_array = np.array(image)\n",
    "    mean = np.mean(img_array)\n",
    "    std = np.std(img_array)\n",
    "    img_array = (img_array - mean) / std\n",
    "    img_array = (img_array - np.min(img_array)) / (np.max(img_array) - np.min(img_array)) * 255\n",
    "\n",
    "    normalized_image = Image.fromarray(img_array.astype('uint8'))\n",
    "\n",
    "    return normalized_image\n",
    "\n",
    "async def resize_image(img, target_size):\n",
    "    \"\"\"\n",
    "    Resizes an image to \n",
    "    the specified target size.\n",
    "    Target size should be like this-> e.g: (200, 200)\n",
    "    \"\"\"\n",
    "    resizedImage=cv2.resize(img, target_size, cv2.INTER_AREA)\n",
    "    return resizedImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. This function bellow takes the faces_list as an input which has generated by the face detection model(YuNet) and extracts the face. w and h in the below code stands for width and height of the driver's face respectively!\n",
    "As you notice, we are doing some preprocessing in face extraction. If our program couldn't detect any faces it will return a numpy array just filled with zeros possessing the same shape of the resized image that it supposed to return which is (150, 150, 3). (150*150 image with three channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def face_extraction(faces_list, img):\n",
    "    \"\"\"\n",
    "    This function withdraws faces from the image\n",
    "    to send as an input to our deep learning model.\n",
    "    \"\"\"\n",
    "    if faces_list[1] is not None:\n",
    "        for face in faces_list[1]:\n",
    "            x, y, w, h = face[0:4].astype(int)\n",
    "            normalized_image = cv2.normalize(\n",
    "                img, \n",
    "                None, \n",
    "                alpha=0, \n",
    "                beta=255, \n",
    "                norm_type=cv2.NORM_MINMAX)\n",
    "            cv2.rectangle(normalized_image, (x, y), (x + w, y + h), (0, 255, 0), 1)\n",
    "\n",
    "            #h>25 and w>20 is because we only are interested in detecting driver's face which is closer to the camera.\n",
    "            if h>25 and w>20:\n",
    "                #licence pictures shouldn't be normalized \n",
    "                roi_color = normalized_image[y:y + h, x:x + w] \n",
    "                try:\n",
    "                    resized_image=await resize_image(roi_color, (150, 150))\n",
    "                    # cv2.imwrite(\"output_extr.jpg\",resized_image)\n",
    "                    return resized_image\n",
    "                except Exception as e:\n",
    "                    # print(\"NO FACE DETECTED\")\n",
    "                    print(e)\n",
    "            else:\n",
    "                # print(f\"NO FACE DETECTED\")\n",
    "                return np.zeros(shape=(150, 150, 3))\n",
    "        else:\n",
    "            return np.zeros(shape=(150, 150, 3))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we are predicting that whether the detected face is the one in the license picture or not! One important thing to consider is that the threshold of this function to return True is 0.9\n",
    "\n",
    "This number is based on the trial and error process I've done on the dataset and got the best results. However my data was too small, I strongly encourage you to regulize this number if you have an extended and large dataset! \n",
    "\n",
    "The resnet model returns a distance parameter as well, which is distance of the embeddings of two images given to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def predict(lisence_image:Image.Image, image:Image.Image):\n",
    "    \"\"\"\n",
    "    This function preprocesses the extracted images\n",
    "    and then gives preprocessed images to the\n",
    "    Resnet deep learning model. the output is\n",
    "    the 'distance'(distance of embeddings of neural network which is float value),\n",
    "    and 'prediction'(true or false, the threshold to have true output is distance<0.9).\n",
    "    I proceed to this number with trial and error.\n",
    "    you can exert some changes to this threshold number4 if needed.  \n",
    "    \"\"\"\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    #image anchor is the licence image\n",
    "    image_anchor = ImageEnhance.Contrast(lisence_image)\n",
    "    image_anchor = image_anchor.enhance(1)\n",
    "    tensor_imgAnchor = transform(image_anchor)\n",
    "\n",
    "    #convert images to opencv object\n",
    "    numpy_image_data = np.array(image)\n",
    "    opencv_image_data = cv2.cvtColor(numpy_image_data, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    #histogram equalization\n",
    "    hist,_ = np.histogram(opencv_image_data.flatten(),256,[0,256])\n",
    "    cdf = hist.cumsum()\n",
    "    # cdf_normalized = cdf * hist.max()/ cdf.max()\n",
    "    cdf_m = np.ma.masked_equal(cdf,0)\n",
    "    cdf_m = (cdf_m - cdf_m.min())*255/(cdf_m.max()-cdf_m.min())\n",
    "    cdf = np.ma.filled(cdf_m,0).astype('uint8')\n",
    "    img_hist = cdf[image]\n",
    "\n",
    "    #convert back to PIL images\n",
    "    pil_image = Image.fromarray(img_hist)\n",
    "\n",
    "    #preprocessing images\n",
    "    image_data_gryscale = ImageOps.grayscale(pil_image)\n",
    "    normalized_image = await normalize_illumination(image_data_gryscale)\n",
    "    denoised_image_data = normalized_image.filter(ImageFilter.MedianFilter(size=3))\n",
    "    \n",
    "    #Get embeddings of my model\n",
    "    tensor_imgData = transform(await grayscale_to_3channels(denoised_image_data))\n",
    "\n",
    "    embeddings_anchor = resnet(tensor_imgAnchor.unsqueeze(0)).detach()\n",
    "    embeddings_data = resnet(tensor_imgData.unsqueeze(0)).detach()\n",
    "\n",
    "    #The threshold needed to get true results for facematch is considered 0.9\n",
    "    distance = (embeddings_anchor - embeddings_data).norm().item()\n",
    "    return distance, distance<0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API\n",
    "As you know, this program has only one method which is post and tries to detect and match people in two pictures. based on functions I have discussed above undestanding this POST method should be a piece of a cake. \n",
    "\n",
    "**Remember** :\n",
    "\n",
    "image_anchor --> License image\n",
    "\n",
    "image_data-->picture that has taken in the bus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post(\"/facematch\")\n",
    "async def facematch_api(image_anchor:UploadFile=File(...), image_data:UploadFile=File(...)):\n",
    "    #Raise error if input files are Null:\n",
    "    if not image_anchor or not image_data:\n",
    "        raise notReceived_exception_handler(name = \"file\")\n",
    "    \n",
    "    #image data types which are mentioned above are allowed data types:\n",
    "    if image_anchor.content_type not in allowed_image_types or image_data.content_type not in allowed_image_types:\n",
    "        raise HTTPException(status_code=400, \n",
    "                            detail=\"Invalid File type. (only jpeg, png or gif are allowed)\"\n",
    "                        )\n",
    "    contents_data = await image_data.read() \n",
    "    contents_anch = await image_anchor.read() \n",
    "    pil_image_data = Image.open(BytesIO(contents_data))\n",
    "    pil_image_anchor = Image.open(BytesIO(contents_anch))\n",
    "\n",
    "    #Some preprocessing on images:\n",
    "    pil_image_data = pil_image_data.rotate(180)\n",
    "    \n",
    "    image_data = await convert2cv2(pil_image_data)\n",
    "    image_anchor = await convert2cv2(pil_image_anchor)\n",
    "    \n",
    "    faces_data = face_detector_data.detect(image_data)\n",
    "    faces_anchor = face_detector_anchor.detect(image_anchor)\n",
    "\n",
    "    extracted_anchor_face = await face_extraction(faces_anchor, image_anchor)\n",
    "    extracted_data_face = await face_extraction(faces_data, image_data)\n",
    "\n",
    "    #if face could not be detected in the image, no face will be extracted. 'face_extraction' function will return np.ndarray of zeros.\n",
    "    if np.all(extracted_data_face==0) or extracted_data_face is None:\n",
    "        return HTTPException(status_code=400, detail=\"No Face Detected in the Picture.(the variable 'exreacted_data_face' is None)\")\n",
    "    extracted_data_face = await convert2PIL(extracted_data_face)\n",
    "    extracted_anchor_face = await convert2PIL(extracted_anchor_face)\n",
    "    _, prediction = await predict(extracted_anchor_face, extracted_data_face)\n",
    "    return {\"pred\":prediction}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note** : \n",
    "\n",
    "My resnet model wasn't trained on the driver's dataset due to insufficient image data of drivers. In order to get more accurate results, I strongly recommand to \"Learn\" or \"Transfer Learn\" on the driver's dataset!\n",
    "\n",
    "\n",
    "\n",
    "Best regards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
